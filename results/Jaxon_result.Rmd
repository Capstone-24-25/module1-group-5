---
title: "Jaxon_Result"
author: "Jaxon Zhang"
date: "2024-10-28"
output: html_document
---

```{r setup, include=FALSE}
library(reticulate)
```

```{python load packages}
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, confusion_matrix
from matplotlib import pyplot as plt
import pickle

with open('../data/q4_variables.pkl', 'rb') as f:
  q4_variables = pickle.load(f)
  
biomarker_rfe_top10_x = q4_variables['biomarker_rfe_top10_x'] 
biomarker_rfe_top10_y =  q4_variables['biomarker_rfe_top10_y']
biomarker_rfe_top10_names = q4_variables['biomarker_rfe_top10_names']
offset_index = q4_variables['offset_index']
label_x = q4_variables['label_x']
label_y = q4_variables['label_y']
biomarker_name = q4_variables['biomarker_name']
offsets_1 = q4_variables['offsets_1']
ranking_sorted = q4_variables['ranking_sorted']
x_axis = q4_variables['x_axis']
y_axis = q4_variables['y_axis']
fi_rf_sorted = q4_variables['fi_rf_sorted']
```

## Brief Data Cleaning

Ados are the metric that is directly related with the outcome variable. Note that only person who is diagnosed of having Autism has ASD score, it is necessary to drop it provides a direct information for the model to distinguish if a person has ASD. Furthermore, our purpose is to identify a protein panel that has significant expression for people having Autism. Including Ados as a predictor does not make sense.

> ADOS diagnostic algorithms consisting of two behavioral domains: Social Affect (SA) and Restricted and Repetitive Behaviors (RRB) were used to determine an ADOS total score, which provides a continuous measure of overall ASD symptom severity.


## Bioinformatics

Our goal is to find a protein panel that can best predict if levels of expression of them can be used to predict if a person has Autism.

### Methods

- Remove the bio-marker that is highly correlated with each other with the threshold `> 0.9`.
- Apply backwards selection using L1 logistic regression to select top 20 features based on feature importance.
- Apply random forest with 5 folds cross validation to select top 20 features based on the average feature importance
- Union the 40 features to use them as the base model, apply backwards selection using elastic net logistic regression (50% L1 and 50% L2) to select a protein panel with 9 bio-markers
- Fit the final model using L1 logistic regression of 6 bio-markers and achieve the ROC-AUC value of 87.29% 

### L1 logistic regression - Recursive Feature Elimination

Logistic Regression is used for the second round of selection. The L1 Lasso Regularization is used to select the bio-markers from the total 1194 bio-markers (after dropping the bio-markers with high correlation). L1 Regularization penalizes the features with the least absolute coefficients, or weights, in the logit, and eventually drives them to zero. Those ones are bio-markers that are considered to be less important. We select total 20 candidates with the highest rank. Top ten bio-markers are plotted for neat visualization purpose.

```{python L1 logistic regression - Recursive Feature Elimination}
fig, ax = plt.subplots(figsize=[10, 6]);
plt.scatter(x=x_axis, y=y_axis, s=1);

for x, y, name, idx in zip(biomarker_rfe_top10_x, biomarker_rfe_top10_y, biomarker_rfe_top10_names, offset_index):
  ax.scatter(biomarker_rfe_top10_x, biomarker_rfe_top10_y, c='red', s=1);

  offset_x = 40 if idx <= 10 else None
  offset_y = (idx % 10) * 12 + 50

  ax.annotate(
    f'{name}' if idx <= 10 else None,
    (x, y),
    textcoords='offset points',
    xytext=(offset_x, offset_y),
    fontsize=6,
    ha='center',
    color='black',
    bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white', alpha=0.5) if idx <= 10 else None,
    arrowprops=dict(arrowstyle='-', color='grey', linewidth=0.5) if idx <= 10 else None
  )

ax.set_xticks(ticks=());
ax.set_xlabel('Protein');
ax.set_ylabel('Ranks (Feature Importance)');
ax.set_title('Recursive Feature Elimination - L1 Logistic Regression')
  
plt.show();
```


### Random Forest - 5 folds Cross Validation

Random forest Algorithm is applied for the third round of selection. The Gini Importance, or the mean decrease in impurity, is used as the metric to determine the contribution of each bio-marker in predicting the outcome. The feature importance for each feature is computed by averaging the total impurity decrease across all the trees in the forest where the feature is used. Another 20 bio-markers are identified as candidates for final selection. The plot demonstrated the top 10 bio-markers which have the highest log average feature importance.

We then union two sets of candidates to serve as the pool of our final selection.

```{python}
fig, ax = plt.subplots(figsize=[10, 6])
ax.scatter(x=range(1194), y=fi_rf_sorted, s=1, alpha=0.8);

for x, y, name, idx in zip(label_x, label_y, biomarker_name, offsets_1):
  offset_x = -60 if idx >= 10 else 0  # Labels for first 10 points to the left, next 10 to the right
  offset_y = (idx % 10) * 10 - 90     # Stagger labels vertically to prevent overlap

  ax.scatter(x, y, c='red', s=1);
  ax.annotate(
    f'{name}' if idx >= 10 else None,
    (x, y),
    textcoords='offset points',
    xytext=(offset_x, offset_y),
    ha='center',
    fontsize=6,
    color='black',
    bbox=dict(boxstyle='round,pad=0.3', edgecolor='black', facecolor='white', linewidth=0.5, alpha=0.5),
    arrowprops=dict(arrowstyle="-", color='gray', lw=0.5) if idx >= 10 else None
  )

ax.set_xticks(ticks=());
ax.set_xlabel('Proteins')
ax.set_ylabel('log Average of 5 folds Feature Importance')
ax.set_title('Feature Importance of Random Forest - 5 Folds CV')

plt.show();
```


### Elastic Net Logistic Regression - Recursive Feature Elimination

50% of L1 regularization and 50% of L2 regularization is applied on the Elastic Net Logistic Regression to recursively select bio-markers with each step, finding a 6 bio-markers protein panel which has the best predictive power. While L1 penalization shrinks the least absolute value of coefficients to zero, L2 handles the multicollinearity by distributing importance across correlated features.


### Final Confusion Matrix

The 6 final selected features (bio-markers) are used to fit on a logistic regression model and test using 20% of the original data set. The ROC-AUC metric is used to evaluate the model performance. We reach the final **87.29%** roc-auc score. It can be interpreted as that the possibility of ranking a randomly chosen ASD patient higher than the possibility of ranking a randomly chosen TD patient. 

```{python confusion Matrix}
cm = np.loadtxt('../data/confusion_matrix.csv', delimiter=',')
fig, ax = plt.subplots(figsize=[8, 8]);
ax.imshow(cm, cmap="Blues");
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted TD', 'Predicted ASD'));
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual TD', 'Actual ASD'));
for i in range(2):
  for j in range(2):
    ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontdict={'fontsize': 20});
ax.grid(False);
ax.set_title('Confusion Matrix of Final Predictions');
plt.show();
```
